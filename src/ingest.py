"""
Knowledge Search - ë°ì´í„° ì„ë² ë”©

Obsidian ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ Vector DBì— ì €ì¥
"""

import json
import openai
from supabase import create_client
from typing import List, Dict, Optional
from pathlib import Path
import hashlib
import tiktoken
import re
from datetime import datetime


class KnowledgeIngest:
    """ë°ì´í„° ì„ë² ë”© ë° ì €ì¥"""
    
    def __init__(self, config_path: str = "config.json"):
        """ì´ˆê¸°í™”"""
        with open(config_path) as f:
            config = json.load(f)
        
        self.config = config
        
        self.supabase = create_client(
            config["supabase"]["url"],
            config["supabase"]["key"]
        )
        
        # Embedding configuration
        self.embedding_provider = config["embedding"]["provider"]
        self.embedding_model = config["embedding"]["model"]
        self.embedding_api_key = config["embedding"]["api_key"]
        
        # Translation configuration
        self.translation_provider = config["translation"]["provider"]
        self.translation_model = config["translation"].get("model", "")
        self.translation_api_key = config["translation"].get("api_key", "")
        
        # Chunking configuration
        self.chunk_size = 512
        self.chunk_overlap = 128
        self.min_chunk_size = 100
        
        # tiktoken encoder
        self.encoding = tiktoken.get_encoding("cl100k_base")
    
    def translate_text(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì˜ì–´ë¡œ ë²ˆì—­
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
        
        Returns:
            ë²ˆì—­ëœ í…ìŠ¤íŠ¸ ë˜ëŠ” ì›ë³¸
        """
        if self.translation_provider == "none":
            return text
        
        try:
            if self.translation_provider == "anthropic":
                from anthropic import Anthropic
                
                anthropic = Anthropic(api_key=self.translation_api_key)
                
                response = anthropic.messages.create(
                    model=self.translation_model,
                    max_tokens=4096,
                    temperature=0.3,
                    messages=[
                        {
                            "role": "user",
                            "content": f"You are a professional translator. Translate the following text to English. Preserve formatting, markdown, and technical terms. Keep it natural and accurate.\n\n{text}"
                        }
                    ]
                )
                
                return response.content[0].text
            
            elif self.translation_provider == "openai":
                import openai as oai
                
                oai.api_key = self.translation_api_key
                
                response = oai.chat.completions.create(
                    model=self.translation_model,
                    max_tokens=4096,
                    temperature=0.3,
                    messages=[
                        {
                            "role": "user",
                            "content": f"You are a professional translator. Translate the following text to English. Preserve formatting, markdown, and technical terms. Keep it natural and accurate.\n\n{text}"
                        }
                    ]
                )
                
                return response.choices[0].message.content
            
            else:
                return text
        
        except Exception as e:
            print(f"      âš ï¸  ë²ˆì—­ ì‹¤íŒ¨, ì›ë¬¸ ì‚¬ìš©: {str(e)[:100]}")
            return text
    
    def get_embedding(self, text: str) -> List[float]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        
        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
        
        Returns:
            ì„ë² ë”© ë²¡í„°
        """
        if self.embedding_provider == "openai":
            openai.api_key = self.embedding_api_key
            
            response = openai.embeddings.create(
                model=self.embedding_model,
                input=text
            )
            return response.data[0].embedding
        
        elif self.embedding_provider == "cohere":
            import cohere
            
            co = cohere.Client(self.embedding_api_key)
            response = co.embed(
                texts=[text],
                model=self.embedding_model,
                input_type="search_document"
            )
            return response.embeddings[0]
        
        else:
            raise ValueError(f"Unknown embedding provider: {self.embedding_provider}")
    
    def chunk_text(self, text: str, metadata: dict) -> List[Dict]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            metadata: ë©”íƒ€ë°ì´í„°
        
        Returns:
            ì²­í¬ ë¦¬ìŠ¤íŠ¸
        """
        # Check word count
        word_count = len(text.split())
        
        # ì§§ì€ ë¬¸ì„œëŠ” ì „ì²´ ì„ë² ë”©
        if word_count < 200:
            return [{
                "text": text,
                "chunk_index": 0,
                "total_chunks": 1,
                **metadata
            }]
        
        # í† í°í™”
        tokens = self.encoding.encode(text)
        
        # ì²­í¬ ë¶„í• 
        chunks = []
        start = 0
        chunk_index = 0
        
        while start < len(tokens):
            end = min(start + self.chunk_size, len(tokens))
            chunk_tokens = tokens[start:end]
            
            # í† í°ì„ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©
            chunk_text = self.encoding.decode(chunk_tokens)
            
            # ìµœì†Œ í¬ê¸° í™•ì¸
            if len(chunk_tokens) >= self.min_chunk_size:
                chunks.append({
                    "text": chunk_text.strip(),
                    "chunk_index": chunk_index,
                    **metadata
                })
                chunk_index += 1
            
            # ë‹¤ìŒ ì²­í¬ ì‹œì‘ (ê²¹ì¹¨ ê³ ë ¤)
            start += (self.chunk_size - self.chunk_overlap)
        
        # total_chunks ì¶”ê°€
        for chunk in chunks:
            chunk["total_chunks"] = len(chunks)
        
        return chunks
    
    def parse_creation_date(self, content: str) -> Optional[str]:
        """
        íŒŒì¼ ë‚´ìš©ì—ì„œ ìƒì„±ì¼ íŒŒì‹±
        
        Args:
            content: íŒŒì¼ ì „ì²´ ë‚´ìš©
        
        Returns:
            ISO 8601 í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ ë˜ëŠ” None
        """
        # Apple Notes í˜•ì‹: "Created: 2016ë…„ 2ì›” 26ì¼ ê¸ˆìš”ì¼ ì˜¤ì „ 2:42:56"
        pattern = r'Created:\s*(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼.*?(\d{1,2}):(\d{2}):(\d{2})'
        match = re.search(pattern, content)
        
        if match:
            try:
                year = int(match.group(1))
                month = int(match.group(2))
                day = int(match.group(3))
                hour = int(match.group(4))
                minute = int(match.group(5))
                second = int(match.group(6))
                
                # ì˜¤ì „/ì˜¤í›„ ì²˜ë¦¬
                if 'ì˜¤í›„' in content[match.start():match.end()+10] and hour < 12:
                    hour += 12
                elif 'ì˜¤ì „' in content[match.start():match.end()+10] and hour == 12:
                    hour = 0
                
                dt = datetime(year, month, day, hour, minute, second)
                return dt.isoformat()
            except:
                pass
        
        return None
    
    def parse_category(self, content: str) -> Optional[str]:
        """
        íŒŒì¼ ë‚´ìš©ì—ì„œ ì¹´í…Œê³ ë¦¬ íŒŒì‹±
        
        Args:
            content: íŒŒì¼ ì „ì²´ ë‚´ìš©
        
        Returns:
            ì¹´í…Œê³ ë¦¬ ë¬¸ìì—´ ë˜ëŠ” None
        """
        # íŒŒì¼ ì²« ì¤„ì—ì„œ "Category: [ì¹´í…Œê³ ë¦¬ëª…]" í˜•ì‹ íŒŒì‹±
        lines = content.split('\n')
        if lines and lines[0].strip().startswith("Category:"):
            category = lines[0].replace("Category:", "").strip()
            return category if category else None
        
        return None
    
    def ingest_file(self, file_path: Path, source: str = "obsidian", author: str = "unknown"):
        """
        íŒŒì¼ì„ ì½ì–´ì„œ ì„ë² ë”©
        
        Args:
            file_path: íŒŒì¼ ê²½ë¡œ
            source: ì†ŒìŠ¤ ì´ë¦„
            author: ì‘ì„±ì
        """
        # íŒŒì¼ ì½ê¸°
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        if not content.strip():
            print(f"   â­ï¸  ë¹ˆ íŒŒì¼: {file_path.name}")
            return
        
        # ìƒì„±ì¼ íŒŒì‹±
        created_date = self.parse_creation_date(content)
        
        # ì¹´í…Œê³ ë¦¬ íŒŒì‹±
        category = self.parse_category(content)
        
        # ë©”íƒ€ë°ì´í„°
        metadata = {
            "path": str(file_path.relative_to(Path.home())),
            "source": source,
            "author": author,
            "folder": file_path.parent.name,
            "file_hash": hashlib.md5(content.encode()).hexdigest()
        }
        
        # ìƒì„±ì¼ì´ ìˆìœ¼ë©´ ì¶”ê°€
        if created_date:
            metadata["created_date"] = created_date
            # date í•„ë“œë„ ì¶”ê°€ (ë‚ ì§œë§Œ, YYYY-MM-DD)
            metadata["date"] = created_date.split("T")[0]
        
        # ì¹´í…Œê³ ë¦¬ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if category:
            metadata["category"] = category
        
        # ì²­í‚¹
        chunks = self.chunk_text(content, metadata)
        print(f"   ğŸ“ {len(chunks)}ê°œ ì²­í¬")
        
        # ê° ì²­í¬ ì²˜ë¦¬
        for i, chunk in enumerate(chunks, 1):
            # ì›ë¬¸ ì €ì¥
            text_original = chunk["text"]
            
            # ë²ˆì—­
            text_translated = self.translate_text(text_original)
            if self.translation_provider != "none":
                print(f"      [{i}/{len(chunks)}] ë²ˆì—­ ì™„ë£Œ")
            
            # ì„ë² ë”©
            embedding = self.get_embedding(text_translated)
            print(f"      [{i}/{len(chunks)}] ì„ë² ë”© ì™„ë£Œ")
            
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            final_metadata = {
                **chunk,
                "text": text_translated,
                "text_original": text_original
            }
            
            # Supabaseì— ì €ì¥
            self.supabase.table("embeddings").insert({
                "embedding": embedding,
                "metadata": final_metadata
            }).execute()
        
        print(f"   âœ… {file_path.name} ì €ì¥ ì™„ë£Œ")
    
    def ingest_folder(self, folder_name: str, source: str = "obsidian", author: str = "unknown"):
        """
        í´ë” ë‚´ ëª¨ë“  .md íŒŒì¼ ì„ë² ë”©
        
        Args:
            folder_name: í´ë” ì´ë¦„
            source: ì†ŒìŠ¤ ì´ë¦„
            author: ì‘ì„±ì
        """
        # Obsidian ê²½ë¡œ
        obsidian_path = Path(self.config["sources"]["obsidian"]["path"]).expanduser()
        folder_path = obsidian_path / folder_name
        
        if not folder_path.exists():
            print(f"âŒ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
            return
        
        # .md íŒŒì¼ ëª©ë¡
        md_files = list(folder_path.glob("*.md"))
        
        if not md_files:
            print(f"âŒ .md íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {folder_name}")
            return
        
        print(f"ğŸ“‚ {folder_name} ({len(md_files)}ê°œ íŒŒì¼)")
        
        for file_path in md_files:
            try:
                self.ingest_file(file_path, source, author)
            except Exception as e:
                print(f"   âŒ ì˜¤ë¥˜: {file_path.name} - {str(e)[:100]}")


def main():
    """CLI ì§„ì…ì """
    import sys
    
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python ingest.py <folder>")
        sys.exit(1)
    
    folder = sys.argv[1]
    
    try:
        ingestor = KnowledgeIngest()
        ingestor.ingest_folder(folder)
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
